---
title: "Creating Text Visualizations with Wikipedia Data"
author: Taylor Arnold
output:
  rmarkdown::html_vignette: default
---

**This document shows the updated version 3 of the package, now available on CRAN**

```{r, echo = FALSE, message=FALSE}
library(magrittr)
library(dplyr)
library(ggplot2)
library(cleanNLP)
library(jsonlite)
library(stringi)
library(xml2)
```

## Grabbing the data

We start by using the MediaWiki API to grab page data from Wikipedia. We will wrap this up
into a small function for re-use later, and start by looking at the English page for oenguins.
The code converts the JSON data into XML data and takes only text within the body of the article.

```{r}
grab_wiki <- function(lang, page) {
  url <- sprintf(
    "https://%s.wikipedia.org/w/api.php?action=parse&format=json&page=%s",
    lang,
    page)
  page_json <- jsonlite::fromJSON(url)$parse$text$"*"
  page_xml <- xml2::read_xml(page_json, asText=TRUE)
  page_text <- xml_text(xml_find_all(page_xml, "//div/p"))
  
  page_text <- stri_replace_all(page_text, "", regex="\\[[0-9]+\\]")
  page_text <- stri_replace_all(page_text, " ", regex="\n")
  page_text <- stri_replace_all(page_text, " ", regex="[ ]+")
  page_text <- page_text[stri_length(page_text) > 10]

  return(page_text)
}

penguin <- grab_wiki("en", "penguin")
penguin[1:10] # just show the first 10 paragraphs
```

## Running the cleanNLP annotation

Next, we run the udpipe annotation backend over the dataset using **cleanNLP**. Because
of the way the data are structured, each paragraph will be treated as its own document.

```{r}
cnlp_init_udpipe()
anno <- cnlp_annotate(penguin, verbose=FALSE)
anno$token
```

## Reconstructing the text

Here, we will show how we can recreate the original text, possibly with additional markings.
This can be useful when building text-based visualization pipelines. For example, let's start
by replacing all of the proper nouns with an all caps version of each word.

```{r}
token <- anno$token
token$new_token <- token$token
change_these <- which(token$xpos %in% c("NNS", "NNPS"))
token$new_token[change_these] <- stri_trans_toupper(token$new_token[change_these])
```

Okay, now we need to know how to put spaces and other details back into the text. This is made possible
by the **space_after** variable:

```{r}
table(token$space_after)
```

We will put these back into the new tokens:

```{r}
token$new_token[token$space_after == "\\n"] <-
  sprintf("%s\n", token$new_token[token$space_after == "\\n"])
token$new_token[token$space_after == "\\s"] <-
  sprintf("%s ", token$new_token[token$space_after == "\\s"])
token$new_token[token$space_after == "\\s\\n"] <-
  sprintf("%s \n", token$new_token[token$space_after == "\\s\\n"])
```

Then, push all of the text back together by paragraph (we use the
`stri_wrap` function to print out the text in a nice format for this
document):

```{r}
paragraphs <- tapply(token$new_token, token$doc_id, paste, collapse="")[1:10]
paragraphs <- stri_wrap(paragraphs, simplify=FALSE, exdent = 1)
cat(unlist(lapply(paragraphs, function(v) c(v, ""))), sep="\n")
```

The **space_after** variable is only provided with the udpipe backend. The spacy
library contains a field with the token that *includes* trailing whitespace. It
is currently not possible to accurately put text back together using the other
two backends.

By outputting the text as HTML or XML, there is a lot of interesting visualization
and metadata work that can be done with this approach. If you have an interesting 
use case that might be useful to others, please feel free to make a pull-request 
to include your work in the package repository.

